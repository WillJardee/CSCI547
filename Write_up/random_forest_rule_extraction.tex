\documentclass[10pt]{article}

\usepackage{amssymb,amsmath, mathtools} 
\usepackage{geometry, graphicx}
\usepackage{algorithm, algpseudocode}
\usepackage{tabulary}
\usepackage{upgreek}
\usepackage{siunitx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{csvsimple}
\usepackage{hanging}
\usepackage[export]{adjustbox}
\usepackage{multirow}
\usepackage{url}

\usepackage{enumitem}
\usepackage{physics}

\usepackage{mathtools}

\usepackage{soul}




\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{it}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


\usepackage{jmlr2e}

\usepackage[noabbrev,capitalize]{cleveref}



% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}


% Short headings should be running head and authors' last names


\ShortHeadings{Random Forest Rule Extraction}{Jardee and Shi}
\firstpageno{1}


\begin{document}


\title{Random Forest Rule Extraction}

\author{\name William Jardee\email willjardee@gmail.com \\
       \addr Department of Physics\\
       Montana State University\\
       Bozeman, MT 59715, USA\AND
       \name Lin Shi\email linshi1768@gmail.com\\
       \addr Giantforte School of Computing\\
       Montana State University\\
       Bozeman, MT 59715, USA
       }
\editor{N/A}

\maketitle

\begin{abstract}%

\end{abstract}
 
\section{Introduction}
\label{sec:intro}


%---------------------------------------------------------------------

\section{Related Works}
\label{sec:related works}
\subsection{Decision trees and random forests}

\subsection{Tree extraction from random forest}

\subsection{Rule representation of neural networks}


%---------------------------------------------------------------------

\section{Rule Extraction from Random Forests}
\label{sec:alg}
\subsection{Co-variance matrix for forest clauses}
Each path in a decision tree can be extracted as a logical rule. Consider a path in a binary decision tree that follows 
\[\neg A \land \neg B \land C \rightarrow W_0 \,,\]
using material implication and De Morgan's law this rule can be written as 
\begin{equation}
\neg(\neg A) \lor \neg (\neg B) \lor \neg(C) \lor W_0 \, .
\label{eq:alg1}
\end{equation}
Notice that we have decided not to cancel out the negations; this will be useful for later as they will cancel out. Each path of each tree in the random forest can be extracted as one of these rules. We propose that if the problem can be explained with high order logical rules, these rules may be embedded in these tree paths and, consequently, in these extracted rules. If two clauses often show up together, we propose that they probably come from the same logical rule.  

For simplicity, all of the features are assumed to be binary. Let us construct a $(2n +c) \times (2n+c)$ matrix of all zeros, where $n$ is the number of features, and $c$ is the number of classes. Each of the rows and columns will correspond to a possible logical decision. For the example in equation~\ref{eq:alg1} the first $2n$ rows would correspond to $[\neg(A), \neg(B), \neg(C), A, B, C]$ and the last $c$ would correspond to the possible classes. Each time a pair of clauses show up together in a rule, $1$ will be added to the corresponding cell. This describes a co-variance matrix.

To illustrate this, see the rule matrix, $\Delta$, for the running example:
\begin{align*}
&\;\:A\:\:\:B\:\:\:C\:\:\:\bar{A}\:\:\:\bar{B}\:\:\:\bar{C}\:W_0\:W_1\\
\Delta = \quad \mqty{A\\B\\C\\\bar{A}\\\bar{B}\\\bar{C}\\W_0\\W_1}\, &\mqty[\cdot&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot\:\\\cdot&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot\\\cdot&\cdot&1&1&1&\cdot&1&\cdot\\\cdot&\cdot&1&1&1&\cdot&1&\cdot\\\cdot&\cdot&1&1&1&\cdot&1&\cdot\\\cdot&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot\\\cdot&\cdot&1&1&1&\cdot&1&\cdot\\\cdot&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot] \, ,
\end{align*}
where $\bar{A} \equiv \neg A$ and zeros have been replaced with $\cdot$ for readability. Adding up the $\Delta$ for each path in the forest gives a complete co-variance matrix. Similar to in Principle Component Analysis (PCA), dimensionality reduction can be done by using eigen decomposition.

\subsection{Rule set extraction from co-variance matrix}
Given the eigendecomposition of the matrix, represented in the form of the set of eigenvalues, $\lambda$, and its corresponding eigenvector, $\vec{v}$, a reduced representation of the covariance matrix can be extracted. Eigenvectors with larger $\lambda$ have more importance to the matrix, as the linear transformation favors that direction. Consequently, the $k$ most important vectors can be chosen by picking the eigenvectors that correspond to the $k$ largest eigenvalues. To pick out only the essential characteristics, the first $2n$ values of the eigenvector and the last $c$ should be pruned individually. This decreases the input noise from the features and picks out the important classes of the rule this vector should describe. 

Because the distribution of the component values is not known, sophisticated models should not be used here; i.e., a one standard deviation cut. For this paper we take the 4th quartile of the features. Future work should look into how to include the weights into rule expression, but we choose to set all rule values to 1 for simplicity. The feature portion of the eigenvector can be explained by the function
\begin{equation}
f_f(x_i)=\begin{cases}
          1 \quad &\text{if} \, x_i > 75\% \text{ of vector} \\
          0 \quad &\text{Otherwise} \\
     \end{cases} \, .
\end{equation}
For the classes portion of the eigenvector, values are kept if their contribution is greater than random. For a normalized vector with equal weighting on all components, each has a value of $1/\sqrt{c}$. For all the values larger than random the value is kept, otherwise it is set to zero. This is explained with the function
\begin{equation}
f_c(x_i)=\begin{cases}
          1 \quad &\text{if} \, x_i > 1/\sqrt{c} \\
          0 \quad &\text{Otherwise} \\
     \end{cases} \, .
\end{equation}
The resulting vector is normalized, and each component is squared to provide a probability measure for each class. This quantitative value is not used in the project's current state, but it could be either reported with the rules to educate how likely each class outcome is or used in a scoring metric.

There is no guarantee that the resulting rules fully cover the class space, which means that the whole problem cannot be explained by the extracted rules so far. The remaining eigenvectors should be searched for rules that cover missing classes to account for this. This is done with a greedy approach that adds the eigenvector with the largest eigenvalue to cover the missing classes. The preferred number of times a class is covered is unclear, as only one rule that describes the class may be insufficient. So, the number of times each class must show up, $k^*$, should be tuned. If $k^*$ is zero, there is no enforcement that each class is covered. 

For an example, take the extracted eigenvector
\[\mqty{\, \\\vec{v} = } \:\mqty{A&B&C&\bar{A}&\bar{B}&\bar{C}&W_0&W_1&W_2&W_3\\(6&5&4&3&2&1&0.3&0.1&0.3&0)}\]
where the number of classes has been increased to four from the previous example. The fourth quartile of the features will be kept (rounding down); i.e., $(1,1,0,0,0,0)$ and the values larger than $0.125$ for the classes will be kept; i.e., $(1, 0, 1, 0)$. But, recall that there is a special procedure for the class vectors, so the class vector is $(0.5, 0, 0.5, 0)$. Putting this together extracts the rule vector 
\[\mqty{\, \\\vec{v}\, ^\prime = } \:\mqty{A&B&C&\bar{A}&\bar{B}&\bar{C}&W_0&W_1&W_2&W_3\\(1&1&0&0&0&0&0.5&0&0.5&0)} \, .\]

The extraction of logical rules from the rule vectors will mirror how they were encoded. That is by a conjunction of OR statements, with the features gaining a negation. The resulting form can be considered a horn statement, where the only positive clause is the union of classes. The eigenvalue information and ratio of classes can be extracted as well. 

Continuing the running example, and assigning the arbitrary eigenvalue of $\lambda =0.8$, the extracted rule would look like
\[\neg(A)\lor \neg(B) \lor W_0 \lor W_1 \qquad \lambda=0.8 \qquad [0.5, 0.5] \, ,\]
\[A \land B \rightarrow (W_0 \lor W_1) \qquad \lambda^2=0.64 \qquad [0.5, 0.5] \, .\]
The square of an eigenvalue, when all the eigenvalues are normalized, corresponds to the percentage of the co-variance matrix that is described by the corresponding eigenvector. Thus, it would make sense to report $\lambda^2$ with the rules as a measure of relative importance. It should be clear now why $k^*$ was introduced. many rules will probably imply the existence of a possible set of classes. $k^*$ greater than one allows more descriptive rule sets.

\begin{algorithm}[!ht]
\caption{RFRE (\textit{Random Forest Rule Extraction})}\label{alg:RFRE}
\begin{algorithmic}[1]
\State {\sl rf } $\gets$ RandomForestGeneration		
\Comment{1. Creating random forest rule-set}
\State {\sl extractedRules} $\gets [\, ]$
\For{$t$ \textbf{in} {\sl rf}}
	\State {\sl treeRules}  $\gets [\,]$
	\For{{\sl rule} \textbf{in} $t$}
		\State {\sl treeRules} $\gets$ {\sl treeRules} $+$ {\sl rule} 
	\EndFor
	\State {\sl extractedRules} $\gets$ {\sl extractedRules} + {\sl treeRules}
\EndFor

\State $n \, \gets$ $($number of features $\times \, 2) \, + \,($number of classes$)$ 
\Comment{2. Creating co-variance matrix for the rule-set}
\State {\sl Map} $\gets \, n\times n$ matrix of zeros
\For{{\sl rule} in {\sl extractedRules}}
	\If{feature $i$ and feature $j$ in {\sl rule}}
		\State {\sl Map}$_{ij} \, \gets$  {\sl Map}$_{ij} + 1$
		\State {\sl Map}$_{ji} \, \gets$  {\sl Map}$_{ji} + 1$
	\EndIf
\EndFor

\State $w, \, v$ $\gets$ Eigen values and vectors of {\sl Map}
\Comment{3. Rule extraction from co-variance matrix}
\State {\sl finalRules} $\gets$ $\{\}$
\For{{\sl vec} in $v$}
	\State newRule $\gets $ \textbf{rule\_creation}({\sl  vec})
	\If{newRule meets add criteria}
	\State {\sl finalRules} $\gets$ {\sl finalRules} $+$ newRule
	\EndIf
\EndFor
\State \Return {\sl finalRules}
\end{algorithmic}
\end{algorithm}

\subsection{Time complexity}
The theoretical time complexity of the algorithm will be given based on the three sections of the algorithm, as can be seen in algorithm~\ref{alg:RFRE}. This derivation will not consider the time required to build the forest. However, from running the empirical tests, the realistic time to run the algorithm is not vanishingly small. Still, the computational cost of the whole process is driven by the number of trees generated during the random forest generation.

Take the following definition of terms
\begin{align*}
D & \equiv \text{ Number of trees in the forest} & n & \equiv \text{ Number of features} \\
l &\equiv \text{ Maximum tree length} & c &\equiv \text{ Number of classes} \, .
\end{align*}
The number of paths in an entire tree will be on the order of $l!$, so the time complexity of creating the complete ruleset will be $\mathcal{O}(l! D)$. For the construction of the covariance matrix, each possible clause of each rule must be checked, so that is $\mathcal{O}(l!D(2n+c)^2)$, assuming that the number of features is larger than the number of classes, $\mathcal{O}(n^2l!D)$. Finally, the rule extraction will be driven by the eigenvalue decomposition of the data, which is roughly $\mathcal{O}(n^{2.3})$ \citep{strang2006linear}. If the number of trees is assumed to be much larger than the number of features, then the time complexity of the whole algorithm becomes $\mathcal{O}(n^2 l! D)$ .
 
%---------------------------------------------------------------------
 
\section{Experimentation}
\label{sec:experiment}
\subsection{Proposed performance metric}
To measure the performance of the derived ruleset with a test set, a new metric was developed. The motivation of this metric is that for a given input vector, $\vb{X}$, and related class, $y$, the importance of rules that the $\vb{X}$ fit need to be weighted heavier than those it does not. The measurement of how well $\vb{X}$ matches up is done through a dot product in Cartesian space (however, there is no reason that a more complicated metric or kernel function could not be applied to the inner-product). To test the negative nodes, i.e. $\bar{A}$, $\vb{X}$ can be subtracted from the unary vector, notated as $\bar{\vb{X}}$. This gives a vector of all the values the vector is \textit{not}. The inner product of $\bar{\vb{X}}$ and $\bar{A}$ will represent how well the vector lines up with the negative clauses of the rule. Since the positive examples are more specific, their weighting should be larger. To account for this, the inner product of the negative clauses is divided by $n-1$, where $n$ is still the number of features. This is specifically in the context of a dot product in Cartesian space; this regularization should be changed if a metric or kernel is introduced. 

To ensure that these two products are on the same magnitude, all the vectors must be normalized before taking the inner product. By the Schwarz Inequality, for two vectors $v$ and $u$ the inner product of the two, $< \, , \, >$, must satisfy 
\[\langle \,u, v \, \rangle \, \leq \,  ||u|| \cdot ||v||\]
\citep{strang2006linear}. If all the vectors are normalized before taking any inner products then $\langle \,\vb{X}, A \, \rangle \,\leq\, 1$ and $\langle \, \bar{\vb{X}}, \bar{A} \, \rangle \, \leq \, 1$. So, for a given rule, the weight can be calculated with 
\[\alpha_i = \langle \,\vb{X}, A \, \rangle + \langle \, \bar{\vb{X}}, \bar{A} \, \rangle / (n-1) \, ,\]
where $\alpha_i \in [0, 1+ 1/(n-1)]$.

If the class $y$ is in the rule's conclusion, $W$, then the two can be thought of a positive example, otherwise it is a negative example;
\begin{equation*}
\beta_i=\begin{cases}
          1 \quad &\text{if} \, y \in W \\
          -1 \quad &\text{if} \, y \notin W\\
     \end{cases} \, .
\end{equation*}
The total weight score of a test instance can then be quantified as 
\begin{equation}
\gamma = \frac{\sum\limits_{i=0}^k f(\alpha_i) \cdot \beta_i}{\sum\limits_{i=0}^k f(\alpha_i)},  \quad \gamma \in [-1, 1]
\label{eq:rule_meas}
\end{equation}
where $f(\alpha_i)$ is a given weight function. The two weight functions tested were the linear map, $f: x\mapsto x$, and the exponential map, $f: x\mapsto \exp(x)$. 


\subsection{Implementation}

\subsection{Data sets}
Considering this project's scope, only three data sets were evaluated to see how well the algorithm performed under various challenges. All three of the data sets were collected from the University of California Irvine, Center for Machine Learning and Intelligent Systems' Machine Learning Repository\footnote{https://archive.ics.uci.edu/ml/index.php}. The data sets consisted of categorical and ordinal features, but every feature was treated as categorical and dealt with via one-hot encoding to satisfy earlier constraints. Future work should expand the algorithm to handle both ordinal and continuous features, which will be brought up in the discussions section.
\subsubsection{Tic-tac-toe}
The Tic-tac-toe data set\footnote{https://archive.ics.uci.edu/ml/datasets/Tic-Tac-Toe+Endgame} has 958 instances with nine features corresponding to the cells of a game of tic-tac-toe. These instances are created by all possible end games where `x' went first\footnote{inspection of the data sets show some games that are not possible but do are consistent with who won the game.}. An instance is classified as `positive' if `x' won and `negative' if `o' won. An example of the classification of four boards can be seen in figure~\ref{fig:ttt_ex}. This data set was chosen because games like tic-tac-toe are defined by a rigorous rule set. The data set is also purely categorical, the assumption that our algorithm works under.
\begin{figure}[ht]
\centering
\begin{tabular}{c c c c}
 1.
	\begin{tabular}{c|c|c}
	x & x & x \\ \hline
	x & o & o \\ \hline
	x & o & o   
	\end{tabular}
	&
2.	
	\begin{tabular}{c|c|c}
	b & b & x \\ \hline
	o & x & o \\ \hline
	x & b & b   
	\end{tabular}
	&
3.
	\begin{tabular}{c|c|c}
	o & b & b \\ \hline
	o & x & b \\ \hline
	o & x & x   
	\end{tabular}
	&
4.
	\begin{tabular}{c|c|c}
	b & b & o \\ \hline
	o & b & o \\ \hline
	o & x & x   
	\end{tabular}
	\\
	(pos) & (pos)& (neg) & (neg)
\end{tabular}

\caption{An example instance from the Tic-tac-toe data set, reformatted to be better understood. `x' and `o' represent each player and `b' represent blank spaces.}
\label{fig:ttt_ex}
\end{figure}

\subsubsection{Car Evaluation}
The Car Evaluation data set\footnote{https://archive.ics.uci.edu/ml/datasets/car+evaluation} evaluates 1728 data instances of cars into four classifications of quality: unacceptable, acceptable, good, and very good. The evaluation metrics for the cars are buying price (low, medium, high, very high), maintenance price (low, medium, high, very high), number of doors (2, 3, 4, 5 or more), person capacity (2, 4, more), luggage boot size (small, medium, big), and estimated safety (low, medium, and high). According to the source, a known concept structure can be extracted from the dataset. However, these are likely harder to deduce than those of the Tic-tac-toe data set. This data set has all ordinal features, so it is expected that the algorithm will have decreased performance. The distribution of classes is very unbalanced, with unacceptable having 70\%, acceptable having 22\%, good 4\%, and very good 4\%, so this will also allow testing on if lower-class presence influences frequency in final rules. 

\subsubsection{Breast Cancer Wisconsin}
The Breast Cancer Wisconsin data set\footnote{https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+\%28original\%29} was tested. Because of points discussed later, the testing was less rigorous on this data set. 699 instances of cancer cells were classified as either benign or malignant with nine features. Each feature was ordinal and spanned from $1-10$. It is expected that casting so many values into a categorical space and a complex structure with few data instances will yield bad performance.

\subsection{Qualitative comparison of rules}
Since the motivation of this algorithm is to represent data instead of providing an alternative model, a qualitative study is motivated. An extensive range of rule sets was generated, and a random sampling of rule sets was taken to parse performance qualitatively. Since this project's scope is minimal, rigorous testing procedures are not in order. All the data can be seen with the written code on GitHub\footnote{https://github.com/WillJardee/CSCI547/; please excuse the mess, as it is still a work in progress and being commented.}. 
\subsubsection{Tic-tac-toe}
The Tic-tac-toe data was tested with rule set size of $k \in \{0, 1, 10, 11, 22\}$. These values were motivated by $0$: only relying on class coverage, $1$: half the average number of feature values, rounded down, $10$: the number of features plus one for the class, $11$: the number of features plus the number of classes, and $22$: double the last number to represent some large number of rules. The required number of instances of a class showing up was $k^* \in \{0,1\}$. Initially, the values were set to $\{0,1,2,3\}$, but a bug in the code meant that only the first two values were tested. Finally, to see how the accuracy of the forest played into the performance, the data was tested on forest sizes $\in \{20, 50, 100, 200, 500, 1000, 2000, 5000\}$. 

The qualitative characteristics of the rule sets can be summarized in the following points:
\begin{enumerate}
\item class coverage becomes more uniform as forests become more accurate,
\item with few classes, there was no difference in requiring class coverage when more rules than the number of classes were used, 
\item rules sets on the order of $10$ rules were the most understandable,
\item in general, the rules are abstract and difficult to understand without domain knowledge.
\end{enumerate}
To justify these claims, see the two small rule set examples (figures~\ref{fig:ttt0-1-20}~and~\ref{fig:ttt0-1-5000}) and the two larger rule set examples (figures~\ref{fig:ttt10-1-20}~and~\ref{fig:ttt10-1-5000}). 

\begin{figure}[!ht]

\begin{subfigure}{\textwidth}
\centering
\begin{tabular}{c c}
	\begin{small}
	\begin{tabular}{c|c|c}
	\hspace*{-0.4em}o/x\hspace*{-0.4em}&\hspace*{-0.4em}o/x\hspace*{-0.4em}&\hspace*{-0.4em}o/x\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}x\hspace*{-0.4em}&\hspace*{-0.4em}o/x\hspace*{-0.4em}&\hspace*{-0.4em}o/b\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}x/b\hspace*{-0.4em}&&
	\end{tabular}
	\end{small}
	&
	\begin{small}
	\begin{tabular}{c|c|c}
	\hspace*{-0.4em}x/b\hspace*{-0.4em}&\hspace*{-0.4em}o/x\hspace*{-0.4em}&\hspace*{-0.4em}o/x\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}x/b\hspace*{-0.4em}&\hspace*{-0.4em}o/x\hspace*{-0.4em}&\hspace*{-0.4em}o/x\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}x/b\hspace*{-0.4em}&&
	\end{tabular}
	\end{small}\\
	1. (pos) &2. (neg)

\end{tabular}
\caption{A Tic-tac-toe rule set from $k=0$, $k^*=1$, forest size $=20$.}
\label{fig:ttt0-1-20}\end{subfigure}
\vspace{1em}

\begin{subfigure}{\textwidth}
\centering
\begin{tabular}{c c}
	\begin{small}
	\begin{tabular}{c|c|c}
	\hspace*{-0.4em}o/x\hspace*{-0.4em}&\hspace*{-0.4em}o/b\hspace*{-0.4em}&  \\ \hline
	\hspace*{-0.4em}o/x\hspace*{-0.4em}&\hspace*{-0.4em}o/x\hspace*{-0.4em}&\hspace*{-0.4em}o/x\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}o/x\hspace*{-0.4em}&\hspace*{-0.4em}b\hspace*{-0.4em}&\hspace*{-0.4em}x/b\hspace*{-0.4em}  
	\end{tabular}
	\end{small}
	&
	\begin{small}
	\begin{tabular}{c|c|c}
	\hspace*{-0.4em}o/x\hspace*{-0.4em}&&\hspace*{-0.4em}x\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}b\hspace*{-0.4em}&\hspace*{-0.4em}b\hspace*{-0.4em}&  \\ \hline
	\hspace*{-0.4em}b\hspace*{-0.4em}&\hspace*{-0.4em}b\hspace*{-0.4em}&\hspace*{-0.4em}o\hspace*{-0.4em}
	\end{tabular}
	\end{small}\\
	1. (pos) &2. (neg)

\end{tabular}

\caption{A Tic-tac-toe rule set from $k=0$, $k^*=1$, forest size $=5000$.}
\label{fig:ttt0-1-5000}
\end{subfigure}
\vspace{1em}

\begin{subfigure}{\textwidth}
\centering
\begin{tabular}{c c c c c}
	\begin{small}
	\begin{tabular}{c|c|c}
	\hspace*{-0.4em}x\hspace*{-0.4em}&\hspace*{-0.4em}x\hspace*{-0.4em}&\hspace*{-0.4em}o\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}b\hspace*{-0.4em}&\hspace*{-0.4em}x\hspace*{-0.4em}&\hspace*{-0.4em}o/x\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}o/x\hspace*{-0.4em}&  &\hspace*{-0.4em}o\hspace*{-0.4em}
	\end{tabular}
	\end{small}
	&
	\begin{small}
	\begin{tabular}{c|c|c}
	\hspace*{-0.4em}x/b\hspace*{-0.4em}&\hspace*{-0.4em}o/x\hspace*{-0.4em}&  \\ \hline
	\hspace*{-0.4em}x/b\hspace*{-0.4em}&\hspace*{-0.4em}x\hspace*{-0.4em}&\hspace*{-0.4em}x\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}o\hspace*{-0.4em}&  &\hspace*{-0.4em}x/b\hspace*{-0.4em}
	\end{tabular}
	\end{small}
	&
	\begin{small}
	\begin{tabular}{c|c|c}
	 &  &  \\ \hline
	\hspace*{-0.4em}o/x\hspace*{-0.4em}&\hspace*{-0.4em}x\hspace*{-0.4em}&\hspace*{-0.4em}o/x\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}x\hspace*{-0.4em}&\hspace*{-0.4em}x\hspace*{-0.4em}&
	\end{tabular}
	\end{small}
	&
	\begin{small}
	\begin{tabular}{c|c|c}
	\hspace*{-0.4em}x/b\hspace*{-0.4em}&\hspace*{-0.4em}o/b\hspace*{-0.4em}&\hspace*{-0.4em}o/b\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}x/b\hspace*{-0.4em}&  &\hspace*{-0.4em}o/x\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}o\hspace*{-0.4em}&  &
	\end{tabular}
	\end{small}
	&
	\begin{small}
	\begin{tabular}{c|c|c}
	\hspace*{-0.4em}x/b\hspace*{-0.4em}&\hspace*{-0.4em}x\hspace*{-0.4em}&\hspace*{-0.4em}o\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}b\hspace*{-0.4em}&\hspace*{-0.4em}b\hspace*{-0.4em}&\hspace*{-0.4em}o\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}x\hspace*{-0.4em}&  &\hspace*{-0.4em}o\hspace*{-0.4em}
	\end{tabular}
	\end{small}
	\\
	1. (neg) &2. (neg) &3. (neg) &4. (neg) &5. (neg) \vspace*{1em}\\

	\begin{small}
	\begin{tabular}{c|c|c}
	 &\hspace*{-0.4em}o/x\hspace*{-0.4em}&\hspace*{-0.4em}o/x\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}x\hspace*{-0.4em}&\hspace*{-0.4em}o/b\hspace*{-0.4em}&\hspace*{-0.4em}o/x\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}o/x\hspace*{-0.4em}&\hspace*{-0.4em}x\hspace*{-0.4em}&\hspace*{-0.4em}o/b\hspace*{-0.4em}
	\end{tabular}
	\end{small}
	&
	\begin{small}
	\begin{tabular}{c|c|c}
	\hspace*{-0.4em}o/b\hspace*{-0.4em}&\hspace*{-0.4em}o/x\hspace*{-0.4em}&  \\ \hline
	\hspace*{-0.4em}o/b\hspace*{-0.4em}&  &\hspace*{-0.4em}b\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}b\hspace*{-0.4em}&\hspace*{-0.4em}o\hspace*{-0.4em}&\hspace*{-0.4em}o/x\hspace*{-0.4em}
	\end{tabular}
	\end{small}
	&
	\begin{small}
	\begin{tabular}{c|c|c}
	 &\hspace*{-0.4em}b\hspace*{-0.4em}&\hspace*{-0.4em}o\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}b\hspace*{-0.4em}&\hspace*{-0.4em}x\hspace*{-0.4em}&\hspace*{-0.4em}b\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}b\hspace*{-0.4em}&  &\hspace*{-0.4em}o/b\hspace*{-0.4em}
	\end{tabular}
	\end{small}
	&
	\begin{small}
	\begin{tabular}{c|c|c}
	 &  &\hspace*{-0.4em}x/b\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}o/x\hspace*{-0.4em}&\hspace*{-0.4em}x/b\hspace*{-0.4em}&\hspace*{-0.4em}o/x\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}x/b\hspace*{-0.4em}&\hspace*{-0.4em}x\hspace*{-0.4em}&\hspace*{-0.4em}o\hspace*{-0.4em}
	\end{tabular}
	\end{small}
	&
	\begin{small}
	\begin{tabular}{c|c|c}
	 &\hspace*{-0.4em}x/b\hspace*{-0.4em}&\hspace*{-0.4em}o/x\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}o/x\hspace*{-0.4em}&\hspace*{-0.4em}x/b\hspace*{-0.4em}&\hspace*{-0.4em}o/x\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}o/x\hspace*{-0.4em}&  &\hspace*{-0.4em}o\hspace*{-0.4em}
	\end{tabular}
	\end{small}
	\\
	6. (neg) &7. (neg) &8. (pos)&9. (pos) &10. (neg)

\end{tabular}

\caption{A Tic-tac-toe rule set from $k=10$, $k^*=1$, forest size $=20$.}
\label{fig:ttt10-1-20}
\end{subfigure}
\vspace{1em}


\begin{subfigure}{\textwidth}
\centering
\begin{tabular}{c c c c c}
	\begin{small}
	\begin{tabular}{c|c|c}
	\hspace*{-0.4em}x\hspace*{-0.4em}&\hspace*{-0.4em}x/b\hspace*{-0.4em}&\hspace*{-0.4em}b\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}b\hspace*{-0.4em}&  &  \\ \hline
	\hspace*{-0.4em}o/b\hspace*{-0.4em}&\hspace*{-0.4em}b\hspace*{-0.4em}&\hspace*{-0.4em}o/x\hspace*{-0.4em}
	\end{tabular}
	\end{small}
	&
	\begin{small}
	\begin{tabular}{c|c|c}
	 &\hspace*{-0.4em}o/x\hspace*{-0.4em}&\hspace*{-0.4em}o/x\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}x\hspace*{-0.4em}&  &  \\ \hline
	\hspace*{-0.4em}x\hspace*{-0.4em}&\hspace*{-0.4em}x\hspace*{-0.4em}&
	\end{tabular}
	\end{small}
	&
	\begin{small}
	\begin{tabular}{c|c|c}
	\hspace*{-0.4em}o/x\hspace*{-0.4em}&\hspace*{-0.4em}b\hspace*{-0.4em}&\hspace*{-0.4em}x\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}b\hspace*{-0.4em}&\hspace*{-0.4em}x\hspace*{-0.4em}&\hspace*{-0.4em}o\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}b\hspace*{-0.4em}&  &\hspace*{-0.4em}b\hspace*{-0.4em}
	\end{tabular}
	\end{small}
	&
	\begin{small}
	\begin{tabular}{c|c|c}
	\hspace*{-0.4em}x/b\hspace*{-0.4em}&\hspace*{-0.4em}o/x\hspace*{-0.4em}&\hspace*{-0.4em}x/b\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}o/x\hspace*{-0.4em}&\hspace*{-0.4em}x\hspace*{-0.4em}&\hspace*{-0.4em}o/b\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}o/x\hspace*{-0.4em}&  &\hspace*{-0.4em}x\hspace*{-0.4em}
	\end{tabular}
	\end{small}
	&
	\begin{small}
	\begin{tabular}{c|c|c}
	 &\hspace*{-0.4em}b\hspace*{-0.4em}&\hspace*{-0.4em}o\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}o\hspace*{-0.4em}&\hspace*{-0.4em}b\hspace*{-0.4em}&\hspace*{-0.4em}x\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}b\hspace*{-0.4em}&\hspace*{-0.4em}x\hspace*{-0.4em}&\hspace*{-0.4em}b\hspace*{-0.4em}
	\end{tabular}
	\end{small}
	\\
	1. (neg) &2. (pos) &3. (pos) &4. (neg) &5. (neg)\vspace*{1em}\\
	\begin{small}
	\begin{tabular}{c|c|c}
	\hspace*{-0.4em}x/b\hspace*{-0.4em}&\hspace*{-0.4em}o\hspace*{-0.4em}&\hspace*{-0.4em}x\hspace*{-0.4em}\\ \hline
	 &  &\hspace*{-0.4em}x\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}b\hspace*{-0.4em}&\hspace*{-0.4em}b\hspace*{-0.4em}&\hspace*{-0.4em}o/x\hspace*{-0.4em}
	\end{tabular}
	\end{small}
	&
	\begin{small}
	\begin{tabular}{c|c|c}
	 &  &\hspace*{-0.4em}o/x\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}o/x\hspace*{-0.4em}&\hspace*{-0.4em}b\hspace*{-0.4em}&\hspace*{-0.4em}o\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}x/b\hspace*{-0.4em}&\hspace*{-0.4em}x/b\hspace*{-0.4em}&\hspace*{-0.4em}b\hspace*{-0.4em}
	\end{tabular}
	\end{small}
	&
	\begin{small}
	\begin{tabular}{c|c|c}
	\hspace*{-0.4em}o/x\hspace*{-0.4em}&  &\hspace*{-0.4em}x/b\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}b\hspace*{-0.4em}&\hspace*{-0.4em}o\hspace*{-0.4em}&\hspace*{-0.4em}o\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}b\hspace*{-0.4em}&  &\hspace*{-0.4em}b\hspace*{-0.4em}
	\end{tabular}
	\end{small}
	&
	\begin{small}
	\begin{tabular}{c|c|c}
	\hspace*{-0.4em}o/x\hspace*{-0.4em}&  &\hspace*{-0.4em}x/b\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}x\hspace*{-0.4em}&\hspace*{-0.4em}o\hspace*{-0.4em}&\hspace*{-0.4em}o\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}o/x\hspace*{-0.4em}&  &    
	\end{tabular}
	\end{small}
	&
	\begin{small}
	\begin{tabular}{c|c|c}
	\hspace*{-0.4em}o/x\hspace*{-0.4em}&\hspace*{-0.4em}x/b\hspace*{-0.4em}&\hspace*{-0.4em}o/x\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}x\hspace*{-0.4em}&\hspace*{-0.4em}o\hspace*{-0.4em}&\hspace*{-0.4em}o\hspace*{-0.4em}\\ \hline
	\hspace*{-0.4em}o/x\hspace*{-0.4em}&\hspace*{-0.4em}o\hspace*{-0.4em}&\hspace*{-0.4em}o/x\hspace*{-0.4em}
	\end{tabular}
	\end{small}
	\\
	6. (pos) &7. (pos) &8. (neg) &9. (neg) &10. (neg)

\end{tabular}

\caption{A Tic-tac-toe rule set from $k=10$, $k^*=1$, forest size $=5000$.}
\label{fig:ttt10-1-5000}
\end{subfigure}

\caption{A collection of rule sets derived from the Tic-tac-toe data set.}
\end{figure}

These visualizations were created by interpreting rules in the form of
\begin{center}
``$(\text{tl}=x) \land \neg(\text{tl}=o) \land \neg(\text{tm}=o) \land (\text{tr}=b) \land (\text{ml}=b) \land \neg(\text{ml}=o) \land (\text{bl}=b) \land \neg(\text{bl}=x) \land (\text{bm}=b) \land \neg(\text{br}=o)  \longrightarrow$ negative  ."
\end{center}
After studying the resulting returned boards, one can convince themselves that there is some underlying structure. However, the visualization on boards of this sort does not aid in learning an abstract rule like ``if three `x' in a row $\leftrightarrow$ positive." There does seem to be some promise in this area with boards like $2$ in figure~\ref{fig:ttt0-1-5000} and several possible `x' or `o' rows in the positive and negative examples, relatively.

\subsubsection{Car Evaluation}
The tests for Car Evaluation were very similar in construction and reasoning as the Tic-tac-toe data set, with $k \in \{0, 2, 6, 10, 20\}$, $k^* \in \{0, 1\}$, and forest size $\in \{20, 50, 100, 200, 500, 1000, 2000, 5000\}$. The bug related to $k^*$ was still present in these tests.

The Car Evaluation data set rules are much harder to interpret and analyze, returning to point four brought up in with the last data set. It seems that points 2, 3, and 4 still hold, but one does not. In most rule sets analyzed, rules with larger eigenvalues tended to include either unacceptable or very good, the two extremes of the spectrum. This makes intuitive sense, as identifying extremes in a data set is an easier task than those near the midpoints. The fact that 70\% of the data set consisted of unacceptable and 4\% of very good seemed to have a small impact on the rule sets generated from small forests and no notable impact on large forests. 

For an example, one rule looked like
\begin{center}
``$\neg$(buying=low)$\land \neg$(maint=vhigh)$\land$(doors=5 more)$\land \neg$(doors=2)$\land$(persons=2)$\land$(lug\_boot=med) $\land \neg$(lug\_boot=big)$\land$(safety=low)$\longrightarrow$unacc or vgood  ."
\end{center}
It may be easier to interpret if the rules are rewritten as 
\begin{center}
[(doors=5 more)$\land$(persons=2)$\land$(lug\_boot=med)$\land$(safety=low)] $\land\neg$[(buying=low)$\lor$(maint=vhigh)$\lor$(doors=2)$\lor$(lug\_boot=big)]$\longrightarrow$ unacc or vgood ,
\end{center}
where the required positive features are grouped, and the required negative features are grouped. It should be understandable how gathering five to ten of these rules soon becomes difficult to track. By searching through many sets of rules, patterns seem to emerge, especially related to unacceptable cars and very good cars. However, this requires many rule sets with different forests and domain knowledge. An example of one of these patterns is the relationship between cars being expensive, having high maintenance costs, and low safety scores with either unacceptable cars or very good cars. This can be thought of as the rules attempting to find the patterns present in both older vehicles, which may be unacceptable, and high-end sports cars considered very good.

\subsubsection{Breast Cancer Wisconsin}
The Breast Cancer Wisconsin data set was run five times with a $k=10$, $k^*=1$ and forest sizes of $20$ and $1000$. In 4 out of the 5 trials and 3 out of 5 trials for the $20$ and $1000$ tree forests, respectively, the first ten rules implied the empty set of classes, and the eleventh rule was just `` $\emptyset \longrightarrow$ benign or malignant." This resulted from the class section of the co-variance matrix is approximately independent of the feature section. The straightforward interpretation of this is that the underlying rules of the data set are too complex to be picked up by our algorithm, as well as the size of the feature space, being on the order of $2^{90}$ when all the features were considered categorical, being too large for the number of classes. Possible approaches to solving this problem will be touched on in the discussions portion.

\subsection{Quantitative study}
To test the performance of the data quantitatively, each trial, as outlined in the qualitative portion, was split into a 70\% trial set and 30\% test set. For each instance, in the testing set, the accuracy measure proposed in equation~\ref{eq:rule_meas} was used, with a linear weight function and an exponential weight function. The performance among every axis of analysis, $k$, $k^*$, and forest size, were practically stochastic; see figure~\ref{fig:quant_stat} for an example. It is unclear whether this response came from the accuracy metric proposed as an incorrect derivation or generally poor algorithm performance. The qualitative section has already partially discussed that patterns did not seem to become evident until rules over multiple forests were considered. If this were the fact, it would make sense that the performance against one rule set would be poor. Since the quantitative results do not give any insight into performance, good or bad, the rest of them will be omitted. 

\begin{figure}[ht]
\centering
\begin{subfigure}{0.49\textwidth}
\includegraphics[width = \textwidth]{images/rules_performance_meas.png}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
\includegraphics[width = \textwidth]{images/forest_performance_meas.png}
\end{subfigure}
\caption{The performance metric, according to equation~\ref{eq:rule_meas}, for the Tic-tac-toe data set. The number of rules and forest size are shown here using a linear weight function (blue/left) and the exponential weight function (orange/right). It can be seen that the performance is stochastic around a small negative value.}
\label{fig:quant_stat}
\end{figure}

%---------------------------------------------------------------------

\section{Discussion}
\label{sec:dis}
\subsection{Contributions and future work}
As can be seen from the experimental results, the general pattern of the algorithm was not optimistic. The algorithm developed was able to pull out some general patterns, especially when patterns were searched for from multiple runs. There was an evidential difference in the rules deduced from better forests, meaning there is a relationship between the quality of the forest structure and the quality of the rules deduced. The algorithm would likely be improved if a couple of crucial facets were addressed:
\begin{enumerate}
\item better statistical analysis of how to construct the covariance matrix and how to weight rules, rule elements, and classes according to eigendecomposition characteristics, 
\item expansion of the algorithm to deal with intervals from ordinal and real-valued features,
\item more sophisticated analysis of the quantitative measure function in equation~\ref{eq:rule_meas}, and
\item expansion of the algorithm to consider relationships between rules from the same tree before adding them to the covariance matrix. 
\end{enumerate}
Since this algorithm did not successfully tackle any of these points, it serves as the starting point for a larger project that could build off the concept outlined here. 

\subsection{Rules as interpretation aid}
Many issues stemmed from not correctly understanding how to use logical rules as an interpretation aid. As discussed in the related works, preexisting methods use rules to understand structures like neural networks. In development, most of the focus was on forest-specific literature and other methods that address representing either trees or forests. To improve this method, guidance should be taken from those related works to address better how to present the ruleset. For example, in theory, giving the counterfactuals of ordinal data points seemed logical, but in testing, it added little understanding to the logical rule. The fundamental algorithm may have shown better performance if the rules were better displayed and a more rigorously backed measure function. 

%---------------------------------------------------------------------

\section{Conclusion}
\label{sec:conc}



%---------------------------------------------------------------------

\vskip 0.2in
\bibliography{random_forest_rule_extraction}

\end{document}
